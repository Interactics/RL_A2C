{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C Actor NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    def __init__(self, action_dim, action_bound):\n",
    "        super(Actor, self).__init__()\n",
    "        self.action_bound = action_bound\n",
    "\n",
    "        self.h1 = Dense(64, activation='relu')\n",
    "        self.h2 = Dense(64, activation='relu')\n",
    "        self.h3 = Dense(16, activation='relu')\n",
    "        self.mu = Dense(action_dim, activation='tanh')\n",
    "        self.std = Dense(action_dim, activation='softplus')\n",
    "\n",
    "    def call(self, states):\n",
    "        x = self.h1(states)\n",
    "        x = self.h2(x)\n",
    "        x = self.h3(x)\n",
    "        mu = self.mu(x)\n",
    "        std = self.std(x)\n",
    "\n",
    "        mu = Lambda(lambda x: x * self.action_bound)(mu)\n",
    "\n",
    "        return [mu, std]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.h1 = Dense(64, activation='relu')\n",
    "        self.h2 = Dense(32, activation='relu')\n",
    "        self.h3 = Dense(16, activation='relu')\n",
    "        self.v = Dense(1, activation='linear')\n",
    "\n",
    "    def call(self, states):\n",
    "        x = self.h1(states)\n",
    "        x = self.h2(x)\n",
    "        x = self.h3(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        return v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2Cagent(object):\n",
    "    def __init__(self, env):\n",
    "        self.GAMMA = 0.99\n",
    "        self.BATCH_SIZE = 32\n",
    "        self.ACTOR_LR = 0.0001\n",
    "        self.CRITIC_LR = 0.001\n",
    "\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        self.action_bound = env.action_space.high[0]\n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.actor = Actor(self.action_dim, self.action_bound)\n",
    "        self.critic = Critic()\n",
    "        self.actor.build(input_shape=(None, self.state_dim))\n",
    "        self.critic.build(input_shape=(None, self.state_dim))\n",
    "\n",
    "        self.actor_opt = Adam(self.ACTOR_LR)\n",
    "        self.critic_opt = Adam(self.CRITIC_LR)\n",
    "\n",
    "        self.save_episode_reward = []\n",
    "\n",
    "    def log_pdf(self, mu, std, actions):\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "        var = std ** 2\n",
    "        log_policy_pdf = -0.5 * (actions - mu) ** 2 / var - 0.5 * tf.math.log(2 * np.pi * var)\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "    \n",
    "\n",
    "    def get_action(self, state):\n",
    "        mu, std = self.actor(state)\n",
    "        mu = mu.numpy()[0]\n",
    "        std = std.numpy()[0]\n",
    "        std = np.clip(std, self.std_bound[0], self.std_bound[1])\n",
    "        action = np.random.normal(mu, std, size=self.action_dim)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def actor_learn(self, states, actions, advantages):\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, std = self.actor(states, training=True)\n",
    "            log_policy_pdf = self.log_pdf(mu, std, actions)\n",
    "\n",
    "            loss_policy = log_policy_pdf * advantages\n",
    "            loss = -tf.reduce_mean(-loss_policy)\n",
    "\n",
    "        grads = tape.gradient(loss, self.actor.trainable_variables)\n",
    "        self.actor_opt.apply_gradients(zip(grads, self.actor.trainable_variables))\n",
    "\n",
    "\n",
    "    def critic_learn(self, states, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            td_hat = self.critic(states, training=True)\n",
    "            loss = tf.reduce_mean(tf.square(td_targets - td_hat))\n",
    "\n",
    "        grads = tape.gradient(loss, self.critic.trainable_variables)\n",
    "        self.critic_opt.apply_gradients(zip(grads, self.critic.trainable_variables))\n",
    "\n",
    "\n",
    "    def td_target(self, rewards, next_v, dones):\n",
    "        y_i = np.zeros(next_v.shape)\n",
    "        for i in range(next_v.shape[0]):\n",
    "            if dones[i]:\n",
    "                y_i[i] = rewards[i]\n",
    "            else:\n",
    "                y_i[i] = rewards[i] + self.GAMMA * next_v[i]\n",
    "        return y_i\n",
    "    \n",
    "    def load_weights(self, path):\n",
    "        actor_weights_path = path + 'pendulum_actor.h5'\n",
    "        critic_weights_path = path + 'pendulum_critic.h5'\n",
    "        self.actor.load_weights(actor_weights_path)\n",
    "        self.critic.load_weights(critic_weights_path)\n",
    "\n",
    "    def unpack_batch(self, batch):\n",
    "        unpack = batch[0]\n",
    "        for idx in range(1, len(batch)):\n",
    "            unpack = np.append(unpack, batch[idx], axis=0)\n",
    "        return unpack\n",
    "    \n",
    "    def train(self, max_episodes=1000):\n",
    "        for ep in range(int(max_episodes)):\n",
    "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = [], [], [], [], []\n",
    "            time, episode_reward, done = 0, 0, False\n",
    "            state = self.env.reset()\n",
    "            state = state[0]\n",
    "            i = 0\n",
    "            while not done:\n",
    "                i= i+1\n",
    "                print (i)\n",
    "                action = self.get_action(tf.convert_to_tensor([state] , dtype=tf.float32))\n",
    "                action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "\n",
    "                state = np.reshape(state, [1, self.state_dim])\n",
    "                action = np.reshape(action, [1, self.action_dim])\n",
    "                reward = np.reshape(reward, [1, 1])\n",
    "                next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                done = np.reshape(done, [1, 1])\n",
    "\n",
    "                train_reward = (reward + 8) / 8\n",
    "\n",
    "                batch_state.append(state)\n",
    "                batch_action.append(action)\n",
    "                batch_reward.append(train_reward)\n",
    "                batch_next_state.append(next_state)\n",
    "                batch_done.append(done)\n",
    "\n",
    "                if len(batch_state) < self.BATCH_SIZE:\n",
    "                    state = next_state[0]\n",
    "                    episode_reward += reward[0]\n",
    "                    time += 1\n",
    "                    continue\n",
    "\n",
    "                states = self.unpack_batch(batch_state)\n",
    "                actions = self.unpack_batch(batch_action)\n",
    "                train_reward = self.unpack_batch(batch_reward)\n",
    "                next_states = self.unpack_batch(batch_next_state)\n",
    "                dones = self.unpack_batch(batch_done)\n",
    "\n",
    "                batch_state, batch_action, batch_reward, batch_next_state, batch_done = [], [], [], [], []\n",
    "\n",
    "                # calculate td_target\n",
    "                next_v = self.critic(tf.convert_to_tensor(next_states, dtype=tf.float32))\n",
    "                td_targets = self.td_target(train_reward, next_v, dones)\n",
    "\n",
    "                # update critic\n",
    "                self.critic_learn(tf.convert_to_tensor(states, dtype=tf.float32), tf.convert_to_tensor(td_targets, dtype=tf.float32))\n",
    "\n",
    "                # calculate advantages\n",
    "                v = self.critic(tf.convert_to_tensor(states, dtype=tf.float32))\n",
    "                next_v = self.critic(tf.convert_to_tensor(next_states, dtype=tf.float32))\n",
    "                advantages = train_reward + self.GAMMA * next_v * (1 - dones) - v\n",
    "\n",
    "                # update actor\n",
    "                self.actor_learn(tf.convert_to_tensor(states, dtype=tf.float32), tf.convert_to_tensor(\n",
    "                    actions, dtype=tf.float32), tf.convert_to_tensor(advantages, dtype=tf.float32))\n",
    "                \n",
    "                state = next_state[0]\n",
    "                episode_reward += reward[0]\n",
    "                time += 1\n",
    "\n",
    "            print('Episode: ', ep, ' Reward: ', episode_reward)\n",
    "            self.save_episode_reward.append(episode_reward)\n",
    "\n",
    "            if ep % 10 == 0:\n",
    "                self.actor.save_weights('./weights/pendulum_actor.h5')\n",
    "                self.critic.save_weights('./weights/pendulum_critic.h5')\n",
    "\n",
    "        np.savetxt('./weights/pendulum_actor.txt', self.save_episode_reward, fmt='%f')\n",
    "        print(self.save_episode_reward)\n",
    "\n",
    "\n",
    "        def plot_result(self):\n",
    "            plt.plot(np.arange(len(self.save_episode_reward)), self.save_episode_reward)\n",
    "            plt.ylabel('Total Reward')\n",
    "            plt.xlabel('Episode')\n",
    "            plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 18:23:44.409561: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x1773a55f0\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "could not find registered platform with id: 0x1773a55f0 [Op:__inference__update_step_xla_1611]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     agent\u001b[39m.\u001b[39mplot_result()\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 13\u001b[0m     main()    \n",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mPendulum-v1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m agent \u001b[39m=\u001b[39m A2Cagent(env)\n\u001b[0;32m----> 8\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(max_episodes)\n\u001b[1;32m     10\u001b[0m agent\u001b[39m.\u001b[39mplot_result()\n",
      "Cell \u001b[0;32mIn[5], line 129\u001b[0m, in \u001b[0;36mA2Cagent.train\u001b[0;34m(self, max_episodes)\u001b[0m\n\u001b[1;32m    126\u001b[0m td_targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtd_target(train_reward, next_v, dones)\n\u001b[1;32m    128\u001b[0m \u001b[39m# update critic\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic_learn(tf\u001b[39m.\u001b[39;49mconvert_to_tensor(states, dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mfloat32), tf\u001b[39m.\u001b[39;49mconvert_to_tensor(td_targets, dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mfloat32))\n\u001b[1;32m    131\u001b[0m \u001b[39m# calculate advantages\u001b[39;00m\n\u001b[1;32m    132\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic(tf\u001b[39m.\u001b[39mconvert_to_tensor(states, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32))\n",
      "Cell \u001b[0;32mIn[5], line 58\u001b[0m, in \u001b[0;36mA2Cagent.critic_learn\u001b[0;34m(self, states, td_targets)\u001b[0m\n\u001b[1;32m     55\u001b[0m     loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(tf\u001b[39m.\u001b[39msquare(td_targets \u001b[39m-\u001b[39m td_hat))\n\u001b[1;32m     57\u001b[0m grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mtrainable_variables)\n\u001b[0;32m---> 58\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic_opt\u001b[39m.\u001b[39;49mapply_gradients(\u001b[39mzip\u001b[39;49m(grads, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic\u001b[39m.\u001b[39;49mtrainable_variables))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1140\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_gradients_aggregation \u001b[39mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[1;32m   1139\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[0;32m-> 1140\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:634\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    633\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, trainable_variables))\n\u001b[0;32m--> 634\u001b[0m iteration \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_apply_gradients(grads_and_vars)\n\u001b[1;32m    636\u001b[0m \u001b[39m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[39mfor\u001b[39;00m variable \u001b[39min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1166\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_internal_apply_gradients\u001b[39m(\u001b[39mself\u001b[39m, grads_and_vars):\n\u001b[0;32m-> 1166\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49m__internal__\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49minterim\u001b[39m.\u001b[39;49mmaybe_merge_call(\n\u001b[1;32m   1167\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distributed_apply_gradients_fn,\n\u001b[1;32m   1168\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distribution_strategy,\n\u001b[1;32m   1169\u001b[0m         grads_and_vars,\n\u001b[1;32m   1170\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(strategy, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[39mreturn\u001b[39;00m distribution_strategy_context\u001b[39m.\u001b[39mget_replica_context()\u001b[39m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1216\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step(grad, var)\n\u001b[1;32m   1215\u001b[0m \u001b[39mfor\u001b[39;00m grad, var \u001b[39min\u001b[39;00m grads_and_vars:\n\u001b[0;32m-> 1216\u001b[0m     distribution\u001b[39m.\u001b[39;49mextended\u001b[39m.\u001b[39;49mupdate(\n\u001b[1;32m   1217\u001b[0m         var, apply_grad_to_update_var, args\u001b[39m=\u001b[39;49m(grad,), group\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m     )\n\u001b[1;32m   1220\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_ema:\n\u001b[1;32m   1221\u001b[0m     _, var_list \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/distribute/distribute_lib.py:2637\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2634\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m   2635\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   2636\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[0;32m-> 2637\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update(var, fn, args, kwargs, group)\n\u001b[1;32m   2638\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2639\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   2640\u001b[0m       var, fn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs, group\u001b[39m=\u001b[39mgroup)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/distribute/distribute_lib.py:3710\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3707\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update\u001b[39m(\u001b[39mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   3708\u001b[0m   \u001b[39m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   3709\u001b[0m   \u001b[39m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 3710\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_non_slot(var, fn, (var,) \u001b[39m+\u001b[39;49m \u001b[39mtuple\u001b[39;49m(args), kwargs, group)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/distribute/distribute_lib.py:3716\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   3712\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_non_slot\u001b[39m(\u001b[39mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   3713\u001b[0m   \u001b[39m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   3714\u001b[0m   \u001b[39m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   3715\u001b[0m   \u001b[39mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 3716\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   3717\u001b[0m     \u001b[39mif\u001b[39;00m should_group:\n\u001b[1;32m   3718\u001b[0m       \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/autograph/impl/api.py:595\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    594\u001b[0m   \u001b[39mwith\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mControlStatusCtx(status\u001b[39m=\u001b[39mag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 595\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1211\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_grad_to_update_var\u001b[39m(var, grad):\n\u001b[1;32m   1210\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit_compile:\n\u001b[0;32m-> 1211\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_step_xla(grad, var, \u001b[39mid\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_var_key(var)))\n\u001b[1;32m   1212\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1213\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step(grad, var)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: could not find registered platform with id: 0x1773a55f0 [Op:__inference__update_step_xla_1611]"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "def main():\n",
    "    max_episodes = 1000\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    agent = A2Cagent(env)\n",
    "\n",
    "    agent.train(max_episodes)\n",
    "\n",
    "    agent.plot_result()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
